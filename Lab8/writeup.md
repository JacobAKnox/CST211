With the last two labs I learned about how sorting algorithms and different data structures perform. Starting with the sorting algorithms, they are all different, but fall into several categories. The ones for lab 7 fall into 5 main categories, partitioning, merging, selection, insertion, and exchanging. Bubble is the only exchange sort, insertion and shell sorts are both insertion, heap and selection are selection sorts, merge sort and quick sort are the only algorithms in their respective categories, merging and partitioning. While they all sort using a different manner, most have average, worst, and best case time complexity of O(nlogn) or O(n^2), though insertion and bubble have a best case of O(n) when the list is already sorted. 
Most have a space complexity that is constant or O(n) if they copy the data to another array. Quick sort O(logn) space complexity if you break up the array, but it can be done in place.

Looking at the measured time complexity, for n=10,000 you would expect a 2500x speed increase over O(n^2) for O(nlogn) or 10000x for O(n). Looking at the data, you only see a 2-3x speed increase for O(nlogn) average time over O(n^2) average. This could be for a number of reasons, the data could be be in a good order for the O(n^2) algorithms, which could lead to a massive performance increase. This seem unlikely as running on a variety of datasets yields similar results. That indicates that the actual bottleneck is processor speed or the data structure. What I gain from this is that, at least on modern hardware, there is not much point in optimizing for a specific sorting algorithm, but the data structure is more important as there is an about 2x speed difference between the speed of c arrays and vectors, and my array class.

Insertion and search from each data structure is much simpler. Arrays, Queues, and Lists are theoretically O(n) search time, BST's are O(logn), and hash tables are O(1). All the inserts are O(1), except for BST's which is O(logn). (Array's are O(n) technically, but since we are only appending it is much closer to O(1)). Based on this you would expect that with 80,000 items insertion time would be about 5x slower for BST's than the other options, with the testing you find it is between 100x (Array's) and 8x (Hash). This is likely due to the extra check to ensure we are not inserting a duplicate entry with a BST the fact that we need to allocate to the heap more frequently than arrays. The main reason that queue insertion is slower than the underlying array implementation is because of the additional checks it needs to perform.  And Hash tables are much slower because hashing strings can take a lot of time. 

Search times are much more consistent with the expected data. Hash tables with O(1) search are the fastest and BST's are not far behind with O(logn). The O(n) search times are lower than expected, only about 30x slower than a BST vs the expected 16000x slower, this is likely due to the fact I picked a word to search for that is relatively early in the text. Looking for a word later in the text was about 700x slower, so it may also be limitations of timing. The queue is about 100x slower than the array, which is likely due to the fact that is has to remove each element before checking the next, there is no reading except for dequeue.